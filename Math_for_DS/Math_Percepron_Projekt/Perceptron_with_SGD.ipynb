{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm_notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Определение функций активации\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Загрузка данных MNIST 70_000 x 784\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "y_train = y_train[:6_000]\n",
    "y_test = y_test[:2_000]\n",
    "# Преобразование данных в нужный формат\n",
    "X_train = X_train.reshape(60_000, 784)[:6_000]\n",
    "X_test = X_test.reshape(10_000, 784)[:2_000]\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "y_train_onehot = np.zeros((y_train.size, y_train.max() + 1))\n",
    "y_train_onehot[np.arange(y_train.size), y_train] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 784) (2000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Best n_hidden: 522\n",
    "# Best learning_rate: 0.1\n",
    "# Best n_epochs: 1500\n",
    "# Best batch_size: 64\n",
    "# Best accuracy: 0.9550\n",
    "\n",
    "# Определение параметров модели\n",
    "n_inputs = X_train.shape[1]   # Количество входных нейронов\n",
    "n_outputs = len(np.unique(y_train))  # Количество выходных нейронов\n",
    "n_hidden = 522 # np.ceil((n_inputs + n_outputs) / 2).astype(int)  # Количество нейронов в скрытом слое\n",
    "learning_rate = 0.1 # Скорость обучения\n",
    "n_epochs = 501 # количество эпох\n",
    "batch_size = 32 # количество образцов данных"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Переменная batch_size определяет количество образцов данных, которые будут использоваться за один раз для обновления весов модели в процессе обучения.\n",
    "Вместо того, что бы использовать все обучающие данные одновременно для каждого шага обновления,\n",
    "мы используем мини-батчи (батчи) данных для более эффективного вычисления градиента и обновления весов.\n",
    "\n",
    "Слишком маленькое значение может привести к тому, что обновление весов будет недостаточно эффективным,\n",
    "из-за того, что маленькие батчи не позволяют модели вычислять градиент на всем объёме данных.\n",
    "С другой стороны, слишком большое значение может привести к нехватке памяти при обучении на больших объёмах данных и снижению эффективности обучения из-за того,\n",
    "что для каждого шага обновления модели требуется больше времени на вычисление градиента.\n",
    "\n",
    "Таким образом, одна эпоха может состоять из нескольких итераций обучения на разных батчах,\n",
    "пока не будут обработаны все обучающие данные. Например, если обучающий набор содержит 50_000 записей,\n",
    "и мы выбираем размер батча в 100, то в каждой эпохе будет 500 итераций обучения на разных батчах.\n",
    "При этом в каждой итерации будут использоваться 100 записей."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------+\n",
      "| Параметр                            |    Значение |\n",
      "+-------------------------------------+-------------+\n",
      "| Количество входных нейронов:        |         784 |\n",
      "| Количество выходных нейронов:       |          10 |\n",
      "| Количество нейронов в скрытом слое: |         522 |\n",
      "| Скорость обучения:                  |         0.1 |\n",
      "| Количество эпох:                    |         501 |\n",
      "| Количество образцов данных:         |          32 |\n",
      "| X_train                             | (6000, 784) |\n",
      "| X_test                              | (2000, 784) |\n",
      "| y_train                             |     (6000,) |\n",
      "| y_test                              |     (2000,) |\n",
      "+-------------------------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "table = [\n",
    "    ['Количество входных нейронов:', n_inputs],\n",
    "    ['Количество выходных нейронов:', n_outputs],\n",
    "    ['Количество нейронов в скрытом слое:', n_hidden],\n",
    "    ['Скорость обучения:', learning_rate],\n",
    "    ['Количество эпох:', n_epochs],\n",
    "    ['Количество образцов данных:', batch_size],\n",
    "    ['X_train', X_train.shape],\n",
    "    ['X_test', X_test.shape],\n",
    "    ['y_train', y_train.shape],\n",
    "    ['y_test', y_test.shape]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers=['Параметр', 'Значение'], tablefmt='pretty', colalign=('left', 'right')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Алгоритм стохастического градиентного спуска (SGD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Алгоритм стохастического градиентного спуска (SGD) - это оптимизационный алгоритм, который используется для обучения нейронных сетей.\n",
    "Он основан на градиентном спуске, который является методом оптимизации для нахождения минимума функции.\n",
    "Шаг оптимизации в алгоритме стохастического градиентного спуска вычисляется на основе градиента функции потерь на каждой итерации.\n",
    "Однако, в отличие от обычного градиентного спуска, который вычисляет градиент по всем обучающим примерам одновременно,\n",
    "SGD вычисляет градиенты на каждой итерации только для одного случайно выбранного обучающего примера.\n",
    "Это позволяет обучению сети быть более быстрым и эффективным."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "По сути, в алгоритме SGD мы делаем следующие шаги:\n",
    "\n",
    "1) Выбираем случайный обучающий пример из обучающей выборки.\n",
    "2) Прямое распространение (forward propagation): вычисляем выход модели на этом примере.\n",
    "3) Обратное распространение (backpropagation): вычисляем градиенты функции потерь на выходном слое и скрытом слое.\n",
    "4) Обновляем веса модели на основе вычисленных градиентов.\n",
    "5) Повторяем шаги 1-4 для каждого обучающего примера в обучающей выборке."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# Инициализация весов\n",
    "weights_input_hidden = np.random.uniform(-0.5, 0.5, size=(n_inputs, n_hidden))  # инициализируем матрицу весов между входным и скрытым слоями случайными значениями из равномерного распределения в диапазоне [-0.5, 0.5].\n",
    "bias_hidden = np.zeros(n_hidden) # инициализируем пороговые значения для скрытого слоя нулями.\n",
    "\n",
    "weights_hidden_output = np.random.uniform(-0.5, 0.5, size=(n_hidden, n_outputs)) # инициализируем матрицу весов между скрытым и выходным слоями случайными значениями из равномерного распределения в диапазоне [-0.5, 0.5].\n",
    "bias_output = np.zeros(n_outputs) # инициализируем пороговые значения для скрытого слоя нулями."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Обучение модели с помощью SGD, градиент вычисляется на небольших случайных подмножествах данных (батчах) в каждую эпоху.\n",
    "for epoch in tqdm_notebook(range(n_epochs)):\n",
    "    # Перемешиваем обучающие данные\n",
    "    indices = np.random.permutation(len(X_train)) # Перемешиваем индексы обучающих данных в случайном порядке, чтобы модель обучалась на случайном наборе данных на каждой эпохе.\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Получаем батч обучающих данных\n",
    "        X_batch = X_train[i:i+batch_size] # Делим обучающие данные на батчи заданного размера.\n",
    "        y_batch = y_train[i:i+batch_size] # Делим обучающие данные на батчи заданного размера.\n",
    "\n",
    "        # Прямое распространение\n",
    "        hidden_inputs = np.dot(X_batch, weights_input_hidden) + bias_hidden # получаем выходные значения скрытого слоя или сумму взвешенных входов для нейронов скрытого слоя.\n",
    "        hidden_outputs = sigmoid(hidden_inputs) # получаем выходные значения скрытого  слоя\n",
    "\n",
    "        output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output # вычисляем взвешенную сумму выходов скрытого слоя и получаем входные значения для выходного слоя.\n",
    "        y_pred = softmax(output_inputs) # получаем предсказания вероятностей для каждого класса.\n",
    "\n",
    "        # Обратное распространение ошибки\n",
    "        error = y_pred - np.eye(n_outputs)[y_batch] # находим ошибку на выходном слое\n",
    "        grad_output = error / len(X_batch) # вычисляем градиент функции потерь по выходному слою.\n",
    "        grad_hidden = np.dot(grad_output, weights_hidden_output.T) * hidden_outputs * (1 - hidden_outputs) # вычисляем градиент функции потерь по скрытому слою.\n",
    "\n",
    "        # Обновление весов и пороговых значений на скрытом и выходном слое\n",
    "        weights_hidden_output -= learning_rate * np.dot(hidden_outputs.T, grad_output)\n",
    "        bias_output -= learning_rate * np.sum(grad_output, axis=0)\n",
    "\n",
    "        weights_input_hidden -= learning_rate * np.dot(X_batch.T, grad_hidden)\n",
    "        bias_hidden -= learning_rate * np.sum(grad_hidden, axis=0)\n",
    "\n",
    "    # Вычисление функции потерь на обучающих и тестовых данных\n",
    "    hidden_train = np.dot(X_train, weights_input_hidden) + bias_hidden # Вычисляем значения скрытого слоя на обучающих данных\n",
    "    hidden_train = sigmoid(hidden_train) # результат применения функции активации сигмоиды к выходному значению скрытого слоя.\n",
    "\n",
    "    output_train = np.dot(hidden_train, weights_hidden_output) + bias_output # Вычисляем значения на выходном слое\n",
    "    y_pred_train = softmax(output_train) # результат применения функции активации softmax к выходному значению выходного слоя, чтобы получить вероятности принадлежности к каждому классу.\n",
    "    train_loss = np.mean(-np.log(y_pred_train[np.arange(len(X_train)), y_train])) # Рассчитываем значение функции потерь на обучающих данных с помощью кросс-энтропии\n",
    "\n",
    "    hidden_test = np.dot(X_test, weights_input_hidden) + bias_hidden # Вычисляем значения скрытого слоя на тестовых данных\n",
    "    hidden_test = sigmoid(hidden_test) # Применяем функцию активации sigmoid к скрытым значениям на тестовых данных\n",
    "\n",
    "    output_test = np.dot(hidden_test, weights_hidden_output) + bias_output # Вычисляем значения на выходном слое для тестовых данных\n",
    "    y_pred_test = softmax(output_test) # вычисляем предсказанные вероятности классов для тестовых данных\n",
    "    test_loss = np.mean(-np.log(y_pred_test[np.arange(len(X_test)), y_test])) # вычисляем значение функции потерь на тестовой выборке с помощью кросс-энтропии\n",
    "\n",
    "    # Выводим значение функции потерь на каждой эпохе\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# [05:25<00:00, 12.261it/s] y_train = y_train[:6000] n_epochs = 500 Accuracy: 0.9155\n",
    "# [28:05<00:00, 11.01it/s] y_train = y_train[:16000] n_epochs = 1000 Accuracy: 0.95225\n",
    "\n",
    "'''\n",
    "+-------------------------------------+--------------+\n",
    "| Параметр                            |     Значение |\n",
    "+-------------------------------------+--------------+\n",
    "| Количество входных нейронов:        |          784 |\n",
    "| Количество выходных нейронов:       |           10 |\n",
    "| Количество нейронов в скрытом слое: |          522 |\n",
    "| Скорость обучения:                  |          0.1 |\n",
    "| Количество эпох:                    |         1501 |\n",
    "| Количество образцов данных:         |           32 |\n",
    "| X_train                             | (15000, 784) |\n",
    "| X_test                              |  (5000, 784) |\n",
    "| y_train                             |     (15000,) |\n",
    "| y_test                              |      (5000,) |\n",
    "+-------------------------------------+--------------+\n",
    "\n",
    "Epoch 1450, Train Loss: 0.0002, Test Loss: 0.2152\n",
    "Epoch 1500, Train Loss: 0.0002, Test Loss: 0.2157\n",
    "Wall time: 49min 49s\n",
    "Accuracy: 0.9524\n",
    "\n",
    "+-------------------------------------+--------------+\n",
    "| Параметр                            |     Значение |\n",
    "+-------------------------------------+--------------+\n",
    "| Количество входных нейронов:        |          784 |\n",
    "| Количество выходных нейронов:       |           10 |\n",
    "| Количество нейронов в скрытом слое: |          397 |\n",
    "| Скорость обучения:                  |         0.05 |\n",
    "| Количество эпох:                    |         1001 |\n",
    "| Количество образцов данных:         |           32 |\n",
    "| X_train                             | (60000, 784) |\n",
    "| X_test                              | (10000, 784) |\n",
    "| y_train                             |     (60000,) |\n",
    "| y_test                              |     (10000,) |\n",
    "+-------------------------------------+--------------+\n",
    "\n",
    "Epoch 1000, Train Loss: 0.0004, Test Loss: 0.0820\n",
    "Wall time: 1h 27min 1s\n",
    "Accuracy: 0.9796\n",
    "\n",
    "\n",
    "\n",
    "после Grid Search:\n",
    "+-------------------------------------+--------------+\n",
    "| Параметр                            |     Значение |\n",
    "+-------------------------------------+--------------+\n",
    "| Количество входных нейронов:        |          784 |\n",
    "| Количество выходных нейронов:       |           10 |\n",
    "| Количество нейронов в скрытом слое: |          522 |\n",
    "| Скорость обучения:                  |          0.1 |\n",
    "| Количество эпох:                    |         1501 |\n",
    "| Количество образцов данных:         |           64 |\n",
    "| X_train                             | (60000, 784) |\n",
    "| X_test                              | (10000, 784) |\n",
    "| y_train                             |     (60000,) |\n",
    "| y_test                              |     (10000,) |\n",
    "+-------------------------------------+--------------+\n",
    "\n",
    "Epoch 1500, Train Loss: 0.0002, Test Loss: 0.0803\n",
    "Wall time: 1h 46min 40s\n",
    "Accuracy: 0.9809\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "вывод значения функции потерь на обучающих и тестовых данных в конце каждой эпохи позволяет обнаружить переобучение модели,\n",
    "когда функция потерь на обучающих данных продолжает уменьшаться, а функция потерь на тестовых данных начинает расти."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Предсказание классов для новых данных\n",
    "hidden_inputs = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
    "hidden_outputs = sigmoid(hidden_inputs)\n",
    "\n",
    "output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output\n",
    "y_pred = np.argmax(output_inputs, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9155\n"
     ]
    }
   ],
   "source": [
    "# Вычисление точности модели на тестовых данных\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid Search для настройки параметров модели:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Определение значений для параметров модели\n",
    "n_inputs = X_train.shape[1]  # Количество входных нейронов\n",
    "n_outputs = len(np.unique(y_train))  # Количество выходных нейронов\n",
    "\n",
    "n_hidden_values = [int(X_train.shape[1]/2), int(X_train.shape[1]/3), int(X_train.shape[1]/1.5), X_train.shape[1]]  # Количество нейронов в скрытом слое [50, 100, 200]\n",
    "learning_rate_values = [0.05, 0.1, 0.25]  # Скорость обучения\n",
    "n_epochs_values = [500, 1000, 1500, 2000]  # Количество эпох обучения\n",
    "batch_size_values = [16, 32, 64]  # Размер батча"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Инициализация переменных для хранения наилучших значений гиперпараметров и метрики качества\n",
    "best_accuracy = 0\n",
    "best_n_hidden = None\n",
    "best_learning_rate = None\n",
    "best_n_epochs = None\n",
    "best_batch_size = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hidden=392, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=392, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9100\n",
      "n_hidden=392, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.9050\n",
      "n_hidden=392, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=392, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=392, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=392, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=392, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=392, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9500\n",
      "n_hidden=392, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
      "n_hidden=392, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=392, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9400\n",
      "n_hidden=261, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9100\n",
      "n_hidden=261, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9150\n",
      "n_hidden=261, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=261, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=261, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=261, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
      "n_hidden=261, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=261, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9050\n",
      "n_hidden=261, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=261, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
      "n_hidden=261, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=261, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9450\n",
      "n_hidden=522, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=522, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=522, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=522, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=522, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=522, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9050\n",
      "n_hidden=522, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9550\n",
      "n_hidden=522, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9100\n",
      "n_hidden=522, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9350\n",
      "n_hidden=522, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9400\n",
      "n_hidden=522, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9100\n",
      "n_hidden=522, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=784, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
      "n_hidden=784, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=784, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.8950\n",
      "n_hidden=784, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=784, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=784, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9250\n",
      "n_hidden=784, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=784, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9100\n",
      "n_hidden=784, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9050\n",
      "n_hidden=784, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9300\n",
      "n_hidden=784, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9150\n",
      "n_hidden=784, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
      "Best n_hidden: 522\n",
      "Best learning_rate: 0.1\n",
      "Best n_epochs: 1500\n",
      "Best batch_size: 64\n",
      "Best accuracy: 0.9550\n",
      "Wall time: 5h 55min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Перебор всех возможных комбинаций параметров\n",
    "for n_hidden in n_hidden_values:\n",
    "    for learning_rate in learning_rate_values:\n",
    "        for n_epochs in n_epochs_values:\n",
    "            for batch_size in batch_size_values:\n",
    "                # Инициализация весов\n",
    "                weights_input_hidden = np.random.uniform(-0.5, 0.5, size=(n_inputs, n_hidden))\n",
    "                bias_hidden = np.zeros(n_hidden)\n",
    "\n",
    "                weights_hidden_output = np.random.uniform(-0.5, 0.5, size=(n_hidden, n_outputs))\n",
    "                bias_output = np.zeros(n_outputs)\n",
    "\n",
    "                # Обучение модели с помощью SGD\n",
    "                for epoch in range(n_epochs):\n",
    "                    # Перемешиваем обучающие данные\n",
    "                    indices = np.random.permutation(len(X_train))\n",
    "                    X_train = X_train[indices]\n",
    "                    y_train = y_train[indices]\n",
    "\n",
    "                    for i in range(0, len(X_train), batch_size):\n",
    "                        # Получаем батч обучающих данных\n",
    "                        X_batch = X_train[i:i+batch_size]\n",
    "                        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "                        # Прямое распространение\n",
    "                        hidden_inputs = np.dot(X_batch, weights_input_hidden) + bias_hidden\n",
    "                        hidden_outputs = sigmoid(hidden_inputs)\n",
    "\n",
    "                        output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output\n",
    "                        y_pred = softmax(output_inputs)\n",
    "\n",
    "                        # Обратное распространение ошибки\n",
    "                        error = y_pred - np.eye(n_outputs)[y_batch]\n",
    "                        grad_output = error / len(X_batch)\n",
    "                        grad_hidden = np.dot(grad_output, weights_hidden_output.T) * hidden_outputs * (1 - hidden_outputs)\n",
    "                        # Обновление весов и пороговых значений\n",
    "                        weights_hidden_output -= learning_rate * np.dot(hidden_outputs.T, grad_output)\n",
    "                        bias_output -= learning_rate * np.sum(grad_output, axis=0)\n",
    "\n",
    "                        weights_input_hidden -= learning_rate * np.dot(X_batch.T, grad_hidden)\n",
    "                        bias_hidden -= learning_rate * np.sum(grad_hidden, axis=0)\n",
    "            # Вычисление точности модели на тестовых данных\n",
    "            hidden_inputs = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
    "            hidden_outputs = sigmoid(hidden_inputs)\n",
    "\n",
    "            output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output\n",
    "            y_pred = np.argmax(output_inputs, axis=1)\n",
    "\n",
    "            accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "            # Обновление наилучших значений гиперпараметров и метрики качества\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_n_hidden = n_hidden\n",
    "                best_learning_rate = learning_rate\n",
    "                best_n_epochs = n_epochs\n",
    "                best_batch_size = batch_size\n",
    "\n",
    "            print(f\"n_hidden={n_hidden}, learning_rate={learning_rate}, n_epochs={n_epochs}, batch_size={batch_size}, Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Best n_hidden: {best_n_hidden}\")\n",
    "print(f\"Best learning_rate: {best_learning_rate}\")\n",
    "print(f\"Best n_epochs: {best_n_epochs}\")\n",
    "print(f\"Best batch_size: {best_batch_size}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "'''\n",
    "Best n_hidden: 522\n",
    "Best learning_rate: 0.1\n",
    "Best n_epochs: 1500\n",
    "Best batch_size: 64\n",
    "Best accuracy: 0.9550\n",
    "Wall time: 5h 55min 41s\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n_hidden_values = [50, 100, 200]  # Количество нейронов в скрытом слое\n",
    "# learning_rate_values = [0.01, 0.1, 0.5]  # Скорость обучения\n",
    "# n_epochs_values = [100, 500, 1000]  # Количество эпох обучения\n",
    "# batch_size_values = [16, 32, 64]  # Размер батча\n",
    "#\n",
    "# n_hidden=50, learning_rate=0.01, n_epochs=100, batch_size=64, Accuracy: 0.6750\n",
    "# n_hidden=50, learning_rate=0.01, n_epochs=500, batch_size=64, Accuracy: 0.8100\n",
    "# n_hidden=50, learning_rate=0.01, n_epochs=1000, batch_size=64, Accuracy: 0.8900\n",
    "# n_hidden=50, learning_rate=0.1, n_epochs=100, batch_size=64, Accuracy: 0.8650\n",
    "# n_hidden=50, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=50, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=50, learning_rate=0.5, n_epochs=100, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=50, learning_rate=0.5, n_epochs=500, batch_size=64, Accuracy: 0.9150\n",
    "# n_hidden=50, learning_rate=0.5, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=100, learning_rate=0.01, n_epochs=100, batch_size=64, Accuracy: 0.7600\n",
    "# n_hidden=100, learning_rate=0.01, n_epochs=500, batch_size=64, Accuracy: 0.8900\n",
    "# n_hidden=100, learning_rate=0.01, n_epochs=1000, batch_size=64, Accuracy: 0.8800\n",
    "# n_hidden=100, learning_rate=0.1, n_epochs=100, batch_size=64, Accuracy: 0.8750\n",
    "# n_hidden=100, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=100, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9150\n",
    "# n_hidden=100, learning_rate=0.5, n_epochs=100, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=100, learning_rate=0.5, n_epochs=500, batch_size=64, Accuracy: 0.9000\n",
    "# n_hidden=100, learning_rate=0.5, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=200, learning_rate=0.01, n_epochs=100, batch_size=64, Accuracy: 0.7700\n",
    "# n_hidden=200, learning_rate=0.01, n_epochs=500, batch_size=64, Accuracy: 0.8950\n",
    "# n_hidden=200, learning_rate=0.01, n_epochs=1000, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=200, learning_rate=0.1, n_epochs=100, batch_size=64, Accuracy: 0.9050\n",
    "# n_hidden=200, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=200, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9400\n",
    "# n_hidden=200, learning_rate=0.5, n_epochs=100, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=200, learning_rate=0.5, n_epochs=500, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=200, learning_rate=0.5, n_epochs=1000, batch_size=64, Accuracy: 0.9250\n",
    "# Best n_hidden: 200\n",
    "# Best learning_rate: 0.1\n",
    "# Best n_epochs: 1000\n",
    "# Best batch_size: 64\n",
    "# Best accuracy: 0.9400"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# n_hidden_values = [int(X_train.shape[1]/2), int(X_train.shape[1]/3), int(X_train.shape[1]/1.5), X_train.shape[1]]\n",
    "# learning_rate_values = [0.05, 0.1, 0.25]  # Скорость обучения\n",
    "# n_epochs_values = [500, 1000, 1500, 2000]  # Количество эпох обучения\n",
    "# batch_size_values = [16, 32, 64]  # Размер батча\n",
    "\n",
    "# n_hidden=392, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=392, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=392, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.9050\n",
    "# n_hidden=392, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=392, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=392, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=392, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=392, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=392, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9500\n",
    "# n_hidden=392, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=392, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=392, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9400\n",
    "# n_hidden=261, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=261, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9150\n",
    "# n_hidden=261, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=261, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=261, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=261, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=261, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=261, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9050\n",
    "# n_hidden=261, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=261, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=261, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=261, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9450\n",
    "# n_hidden=522, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=522, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=522, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=522, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=522, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=522, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9050\n",
    "# n_hidden=522, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9550\n",
    "# n_hidden=522, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=522, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9350\n",
    "# n_hidden=522, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9400\n",
    "# n_hidden=522, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=522, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=784, learning_rate=0.05, n_epochs=500, batch_size=64, Accuracy: 0.9200\n",
    "# n_hidden=784, learning_rate=0.05, n_epochs=1000, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=784, learning_rate=0.05, n_epochs=1500, batch_size=64, Accuracy: 0.8950\n",
    "# n_hidden=784, learning_rate=0.05, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=784, learning_rate=0.1, n_epochs=500, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=784, learning_rate=0.1, n_epochs=1000, batch_size=64, Accuracy: 0.9250\n",
    "# n_hidden=784, learning_rate=0.1, n_epochs=1500, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=784, learning_rate=0.1, n_epochs=2000, batch_size=64, Accuracy: 0.9100\n",
    "# n_hidden=784, learning_rate=0.25, n_epochs=500, batch_size=64, Accuracy: 0.9050\n",
    "# n_hidden=784, learning_rate=0.25, n_epochs=1000, batch_size=64, Accuracy: 0.9300\n",
    "# n_hidden=784, learning_rate=0.25, n_epochs=1500, batch_size=64, Accuracy: 0.9150\n",
    "# n_hidden=784, learning_rate=0.25, n_epochs=2000, batch_size=64, Accuracy: 0.9250\n",
    "# Best n_hidden: 522\n",
    "# Best learning_rate: 0.1\n",
    "# Best n_epochs: 1500\n",
    "# Best batch_size: 64\n",
    "# Best accuracy: 0.9550\n",
    "# Wall time: 5h 55min 41s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}