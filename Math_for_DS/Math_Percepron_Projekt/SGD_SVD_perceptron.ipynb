{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# импортируем нужные библиотеки\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Определение функций активации\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Загрузка данных MNIST 70_000 x 784\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "y_train = y_train[:]\n",
    "y_test = y_test[:]\n",
    "# Преобразование данных в нужный формат\n",
    "X_train = X_train.reshape(60_000, 784)[:]\n",
    "X_test = X_test.reshape(10_000, 784)[:]\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "y_train_onehot = np.zeros((y_train.size, y_train.max() + 1))\n",
    "y_train_onehot[np.arange(y_train.size), y_train] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Определение параметров модели\n",
    "n_inputs = X_train.shape[1]   # Количество входных нейронов\n",
    "n_outputs = len(np.unique(y_train))  # Количество выходных нейронов\n",
    "n_hidden = 522 # np.ceil((n_inputs + n_outputs) / 2).astype(int)  # Количество нейронов в скрытом слое\n",
    "learning_rate = 0.1 # Скорость обучения\n",
    "n_epochs = 101 # количество эпох\n",
    "batch_size = 32 # количество образцов данных"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "\n",
    "# Инициализация весов\n",
    "weights_input_hidden = np.random.uniform(-0.5, 0.5, size=(n_inputs, n_hidden))\n",
    "bias_hidden = np.zeros(n_hidden)\n",
    "\n",
    "weights_hidden_output = np.random.uniform(-0.5, 0.5, size=(n_hidden, n_outputs))\n",
    "bias_output = np.zeros(n_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/101 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72d0a7f9a23b4e408163f776e48130a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 2.1027, Test Loss: 2.5842\n",
      "Epoch 50, Train Loss: 0.0665, Test Loss: 0.4880\n",
      "Epoch 100, Train Loss: 0.0304, Test Loss: 0.4713\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Обучение модели с помощью SGD\n",
    "for epoch in tqdm_notebook(range(n_epochs)):\n",
    "    # Перемешиваем обучающие данные\n",
    "    indices = np.random.permutation(len(X_train)) # перемешиваем индексы обучающих данных\n",
    "    X_train = X_train[indices]  # переупорядочиваем обучающие данные\n",
    "    y_train = y_train[indices] # переупорядочиваем метки классов обучающей выборки\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size): # проходим по всем образцам обучающих данных\n",
    "        # Получаем батч обучающих данных\n",
    "        X_batch = X_train[i:i+batch_size] # выделяем батч обучающих данных\n",
    "        y_batch = y_train[i:i+batch_size] # выделяем метки классов батча обучающих данных\n",
    "\n",
    "        # Прямое распространение\n",
    "        hidden_weights = weights_input_hidden\n",
    "\n",
    "        # Применение SVD для сжатия матрицы весов скрытого слоя\n",
    "        U, s, V = np.linalg.svd(hidden_weights, full_matrices=False) # применяем SVD для разложения матрицы весов скрытого слоя на сингулярные значения\n",
    "\n",
    "        k = 300  # Количество компонент для сжатия\n",
    "\n",
    "        S = np.diag(s[:k]) # создаем диагональную матрицу с сингулярными значениями\n",
    "        U = U[:, :k] # выбираем первые k столбцов матрицы U\n",
    "        V = V[:k, :] # выбираем первые k строк матрицы V\n",
    "        hidden_weights_compressed = np.dot(U, np.dot(S, V)) # вычисляем новую сжатую матрицу весов скрытого слоя\n",
    "\n",
    "        hidden_inputs = np.dot(X_batch, hidden_weights_compressed) + bias_hidden # вычисляем взвешенную сумму входных данных и сжатых весов скрытого слоя, добавляем пороговые значения скрытого слоя\n",
    "        hidden_outputs = sigmoid(hidden_inputs)  # применяем функцию активации сигмоид к взвешенной сумм\n",
    "\n",
    "        output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output # вычисляем взвешенную сумму скрытых выходных значений и весов выходного слоя, добавляем пороговые значения выходного слоя\n",
    "        y_pred = softmax(output_inputs) # применяем функцию активации софтмакс к взвешенной сумме\n",
    "\n",
    "        # Обратное распространение ошибки\n",
    "        error = y_pred - np.eye(n_outputs)[y_batch] # вычисляем ошибку на выходном слое\n",
    "        grad_output = error / len(X_batch) # вычисляем градиент на выходном слое\n",
    "        grad_hidden = np.dot(grad_output, weights_hidden_output.T) * hidden_outputs * (1 - hidden_outputs) # вычисляем градиент на скрытом слое\n",
    "\n",
    "        output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output # вычисляем взвешенную сумму скрытых выходных значений и весов выходного слоя, добавляем пороговые значения выходного слоя\n",
    "        y_pred = softmax(output_inputs) # применяем функцию активации софтмакс к взвешенной сумме\n",
    "\n",
    "        # Обновление весов и пороговых значений\n",
    "        weights_hidden_output -= learning_rate * np.dot(hidden_outputs.T, grad_output) # обновляем веса выходного слоя\n",
    "        bias_output -= learning_rate * np.sum(grad_output, axis=0) # обновляем пороговые значения выходного слоя\n",
    "\n",
    "        weights_input_hidden -= learning_rate * np.dot(X_batch.T, grad_hidden) # обновляем веса скрытого слоя\n",
    "        bias_hidden -= learning_rate * np.sum(grad_hidden, axis=0) # обновляем пороговые значения скрытого слоя\n",
    "\n",
    "    # Вычисление функции потерь на обучающих и тестовых данных\n",
    "    hidden_train = np.dot(X_train, weights_input_hidden) + bias_hidden # вычисляем взвешенную сумму входных данных и весов скрытого слоя, добавляем пороговые значения скрытого слоя\n",
    "    hidden_train = sigmoid(hidden_train) # применяем функцию активации сигмоид к взвешенной сумме\n",
    "\n",
    "    output_train = np.dot(hidden_train, weights_hidden_output) + bias_output # вычисляем взвешенную сумму скрытых выходных значений и весов выходного слоя, добавляем пороговые значения выходного слоя\n",
    "    y_pred_train = softmax(output_train)  # применяем функцию активации софтмакс к взвешенной сумме\n",
    "\n",
    "    train_loss = np.mean(-np.log(y_pred_train[np.arange(len(X_train)), y_train])) # вычисляем функцию потерь на обучающих данных\n",
    "\n",
    "    hidden_test = np.dot(X_test, weights_input_hidden) + bias_hidden # вычисляем взвешенную сумму входных данных и весов скрытого слоя, добавляем пороговые значения скрытого слоя\n",
    "    hidden_test = sigmoid(hidden_test) # применяем функцию активации сигмоид к взвешенной сумме\n",
    "\n",
    "    output_test = np.dot(hidden_test, weights_hidden_output) + bias_output # вычисляем взвешенную сумму скрытых выходных значений и весов выходного слоя, добавляем пороговые значения выходного слоя\n",
    "    y_pred_test = softmax(output_test) # применяем функцию активации софтмакс к взвешенной сумме\n",
    "    test_loss = np.mean(-np.log(y_pred_test[np.arange(len(X_test)), y_test])) # вычисляем функцию потерь на тестовых данных\n",
    "\n",
    "    # Выводим значение функции потерь на каждой эпохе\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8633333333333333\n"
     ]
    }
   ],
   "source": [
    "# Предсказание классов для новых данных\n",
    "hidden_inputs = np.dot(X_test, weights_input_hidden) + bias_hidden # вычисляем взвешенную сумму входных данных и весов скрытого слоя, добавляем пороговые значения скрытого слоя\n",
    "hidden_outputs = sigmoid(hidden_inputs) # применяем функцию активации сигмоид к взвешенной сумме\n",
    "\n",
    "output_inputs = np.dot(hidden_outputs, weights_hidden_output) + bias_output # вычисляем взвешенную сумму скрытых выходных значений и весов выходного слоя, добавляем пороговые значения выходного слоя\n",
    "y_pred = np.argmax(output_inputs, axis=1) # находим индексы классов с максимальными значениями вероятности\n",
    "\n",
    "# Вычисление точности модели на тестовых данных\n",
    "accuracy = np.mean(y_pred == y_test) # находим процент правильно предсказанных классов на тестовых данных\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "no SVD y_train = y_train[:1200] n_epochs = 1500 Accuracy: 0.84\n",
    "50/50 [03:27<00:00, 11.01it/s] y_train = y_train[:1200] n_epochs = 500 к=50 Accuracy: 0.828\n",
    "100/100 [03:27<00:00, 11.01it/s] y_train = y_train[:1200] n_epochs = 100 к=100 Accuracy: 0.85\n",
    "500/500 [03:27<00:00, 11.01it/s] y_train = y_train[:1500] n_epochs = 500 к=250 Accuracy: 0.886\n",
    "500/500 [03:27<00:00, 11.01it/s] y_train = y_train[:2500] n_epochs = 500 к=250 Accuracy: 0.871\n",
    "500/500 [16:27<00:00, 11.01it/s] y_train = y_train[:1200] n_epochs = 500 к=100 Accuracy: 0.854\n",
    "1501/1501 [03:27<00:00, 11.01it/s] y_train = y_train[:1200] n_epochs = 1501 к=100 Accuracy: 0.866\n",
    "500/500 [03:27<00:00, 11.01it/s] y_train = y_train[:5500] n_epochs = 1501 к=100 Accuracy: 0.894\n",
    "500/500 [03:27<00:00, 11.01it/s] y_train = y_train[:2500] n_epochs = 1501 к=550 Accuracy: 0.896\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Далее идут результаты"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SVD On\n",
    "k = 100  # Количество компонент для сжатия\n",
    "# # Определение параметров модели\n",
    "# n_inputs = X_train.shape[1]   # Количество входных нейронов\n",
    "# n_outputs = len(np.unique(y_train))  # Количество выходных нейронов\n",
    "# n_hidden = 522 # np.ceil((n_inputs + n_outputs) / 2).astype(int)  # Количество нейронов в скрытом слое\n",
    "# learning_rate = 0.1 # Скорость обучения\n",
    "# n_epochs = 1501 # количество эпох\n",
    "# batch_size = 32 # количество образцов данных\n",
    "# Epoch 0, Train Loss: 1.3839, Test Loss: 1.6510\n",
    "# Epoch 50, Train Loss: 0.3226, Test Loss: 0.7049\n",
    "# Epoch 100, Train Loss: 0.2353, Test Loss: 0.6711\n",
    "# Epoch 150, Train Loss: 0.2131, Test Loss: 0.6851\n",
    "# Epoch 200, Train Loss: 0.1888, Test Loss: 0.6803\n",
    "# Epoch 250, Train Loss: 0.1727, Test Loss: 0.6811\n",
    "# Epoch 300, Train Loss: 0.1571, Test Loss: 0.6768\n",
    "# Epoch 350, Train Loss: 0.1501, Test Loss: 0.6812\n",
    "# Epoch 400, Train Loss: 0.1416, Test Loss: 0.6817\n",
    "# Epoch 450, Train Loss: 0.1347, Test Loss: 0.6821\n",
    "# Epoch 500, Train Loss: 0.1288, Test Loss: 0.6831\n",
    "# Epoch 550, Train Loss: 0.1237, Test Loss: 0.6840\n",
    "# Epoch 600, Train Loss: 0.1188, Test Loss: 0.6846\n",
    "# Epoch 650, Train Loss: 0.1142, Test Loss: 0.6848\n",
    "# Epoch 700, Train Loss: 0.1103, Test Loss: 0.6851\n",
    "# Epoch 750, Train Loss: 0.1073, Test Loss: 0.6868\n",
    "# Epoch 800, Train Loss: 0.1050, Test Loss: 0.6888\n",
    "# Epoch 850, Train Loss: 0.1026, Test Loss: 0.6901\n",
    "# Epoch 900, Train Loss: 0.0997, Test Loss: 0.6905\n",
    "# Epoch 950, Train Loss: 0.0974, Test Loss: 0.6917\n",
    "# Epoch 1000, Train Loss: 0.0954, Test Loss: 0.6928\n",
    "# Epoch 1050, Train Loss: 0.0934, Test Loss: 0.6936\n",
    "# Epoch 1100, Train Loss: 0.0917, Test Loss: 0.6948\n",
    "# Epoch 1150, Train Loss: 0.0899, Test Loss: 0.6958\n",
    "# Epoch 1200, Train Loss: 0.0881, Test Loss: 0.6963\n",
    "# Epoch 1250, Train Loss: 0.0868, Test Loss: 0.6973\n",
    "# Epoch 1300, Train Loss: 0.0854, Test Loss: 0.6983\n",
    "# Epoch 1350, Train Loss: 0.0839, Test Loss: 0.6990\n",
    "# Epoch 1400, Train Loss: 0.0829, Test Loss: 0.7002\n",
    "# Epoch 1450, Train Loss: 0.0815, Test Loss: 0.7008\n",
    "# Epoch 1500, Train Loss: 0.0807, Test Loss: 0.7021\n",
    "Wall time: 57min 34s\n",
    "Accuracy: 0.866"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 250\n",
    "\n",
    "# Epoch 0, Train Loss: 1.0804, Test Loss: 1.3565\n",
    "# Epoch 50, Train Loss: 0.0981, Test Loss: 0.5274\n",
    "# Epoch 100, Train Loss: 0.0671, Test Loss: 0.5420\n",
    "# Epoch 150, Train Loss: 0.0480, Test Loss: 0.5320\n",
    "# Epoch 200, Train Loss: 0.0401, Test Loss: 0.5353\n",
    "# Epoch 250, Train Loss: 0.0348, Test Loss: 0.5368\n",
    "# Epoch 300, Train Loss: 0.0303, Test Loss: 0.5361\n",
    "# Epoch 350, Train Loss: 0.0275, Test Loss: 0.5384\n",
    "# Epoch 400, Train Loss: 0.0255, Test Loss: 0.5405\n",
    "# Epoch 450, Train Loss: 0.0236, Test Loss: 0.5421\n",
    "# Epoch 500, Train Loss: 0.0219, Test Loss: 0.5425\n",
    "# Wall time: 18min 48s\n",
    "\n",
    "Accuracy: 0.85"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 50  # Количество компонент для сжатия\n",
    "\n",
    "# Epoch 0, Train Loss: 1.3601, Test Loss: 1.6002\n",
    "# Epoch 50, Train Loss: 0.7355, Test Loss: 1.2752\n",
    "# Epoch 100, Train Loss: 0.4583, Test Loss: 1.0451\n",
    "# Epoch 150, Train Loss: 0.3726, Test Loss: 0.9839\n",
    "# Epoch 200, Train Loss: 0.3426, Test Loss: 0.9751\n",
    "# Epoch 250, Train Loss: 0.2963, Test Loss: 0.9399\n",
    "# Epoch 300, Train Loss: 0.2588, Test Loss: 0.9129\n",
    "# Epoch 350, Train Loss: 0.1831, Test Loss: 0.8367\n",
    "# Epoch 400, Train Loss: 0.1484, Test Loss: 0.8013\n",
    "# Epoch 450, Train Loss: 0.1218, Test Loss: 0.7711\n",
    "# Epoch 500, Train Loss: 0.1040, Test Loss: 0.7506\n",
    "# Wall time: 18min 57s\n",
    "\n",
    "Accuracy: 0.828"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 500  # Количество компонент для сжатия\n",
    "# y_train = y_train[:5500]\n",
    "# y_test = y_test[:1000]\n",
    "# Epoch 0, Train Loss: 0.9050, Test Loss: 1.1975\n",
    "# Epoch 50, Train Loss: 0.0332, Test Loss: 0.3967\n",
    "# Epoch 100, Train Loss: 0.0152, Test Loss: 0.3999\n",
    "# Epoch 150, Train Loss: 0.0095, Test Loss: 0.4015\n",
    "# Epoch 200, Train Loss: 0.0069, Test Loss: 0.4068\n",
    "# Epoch 250, Train Loss: 0.0054, Test Loss: 0.4102\n",
    "# Epoch 300, Train Loss: 0.0044, Test Loss: 0.4133\n",
    "# Epoch 350, Train Loss: 0.0038, Test Loss: 0.4170\n",
    "# Epoch 400, Train Loss: 0.0032, Test Loss: 0.4196\n",
    "# Epoch 450, Train Loss: 0.0028, Test Loss: 0.4227\n",
    "# Epoch 500, Train Loss: 0.0025, Test Loss: 0.4248\n",
    "# CPU times: user 2h 26min 39s, sys: 33min 17s, total: 2h 59min 56s\n",
    "# Wall time: 1h 43min 21s\n",
    "Accuracy: 0.894"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = y_train[:1500]\n",
    "y_test = y_test[:500]\n",
    "# Преобразование данных в нужный формат\n",
    "X_train = X_train.reshape(60_000, 784)[:1500]\n",
    "X_test = X_test.reshape(10_000, 784)[:500]\n",
    "n_hidden = 522 # np.ceil((n_inputs + n_outputs) / 2).astype(int)  # Количество нейронов в скрытом слое\n",
    "learning_rate = 0.1 # Скорость обучения\n",
    "n_epochs = 501 # количество эпох\n",
    "batch_size = 32 # количество образцов данных\n",
    "k = 250  # Количество компонент для сжатия\n",
    "# Epoch 0, Train Loss: 1.0307, Test Loss: 1.1796\n",
    "# Epoch 50, Train Loss: 0.0986, Test Loss: 0.4407\n",
    "# Epoch 100, Train Loss: 0.0535, Test Loss: 0.4412\n",
    "# Epoch 150, Train Loss: 0.0382, Test Loss: 0.4425\n",
    "# Epoch 200, Train Loss: 0.0314, Test Loss: 0.4499\n",
    "# Epoch 250, Train Loss: 0.0259, Test Loss: 0.4532\n",
    "# Epoch 300, Train Loss: 0.0236, Test Loss: 0.4608\n",
    "# Epoch 350, Train Loss: 0.0207, Test Loss: 0.4633\n",
    "# Epoch 400, Train Loss: 0.0190, Test Loss: 0.4681\n",
    "# Epoch 450, Train Loss: 0.0178, Test Loss: 0.4714\n",
    "# Epoch 500, Train Loss: 0.0163, Test Loss: 0.4744\n",
    "# CPU times: user 4h 14min 39s, sys: 1h 34min 55s, total: 5h 49min 34s\n",
    "# Wall time: 1h 29min 56s\n",
    "Accuracy: 0.886"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " k = 250  # Количество компонент для сжатия\n",
    "# y_train = y_train[:2500]\n",
    "# y_test = y_test[:1000]\n",
    "# # Преобразование данных в нужный формат\n",
    "# X_train = X_train.reshape(60_000, 784)[:2500]\n",
    "# X_test = X_test.reshape(10_000, 784)[:1000]\n",
    "# Epoch 0, Train Loss: 1.6045, Test Loss: 1.9372\n",
    "# Epoch 50, Train Loss: 0.0835, Test Loss: 0.4695\n",
    "# Epoch 100, Train Loss: 0.0468, Test Loss: 0.4685\n",
    "# Epoch 150, Train Loss: 0.0370, Test Loss: 0.4827\n",
    "# Epoch 200, Train Loss: 0.0265, Test Loss: 0.4807\n",
    "# Epoch 250, Train Loss: 0.0233, Test Loss: 0.4863\n",
    "# Epoch 300, Train Loss: 0.0207, Test Loss: 0.4917\n",
    "# Epoch 350, Train Loss: 0.0182, Test Loss: 0.4947\n",
    "# Epoch 400, Train Loss: 0.0166, Test Loss: 0.4968\n",
    "# Epoch 450, Train Loss: 0.0151, Test Loss: 0.5000\n",
    "# Epoch 500, Train Loss: 0.0143, Test Loss: 0.5036\n",
    "# CPU times: user 3h 48min 47s, sys: 56min 33s, total: 4h 45min 20s\n",
    "# Wall time: 2h 44min 52s\n",
    "Accuracy: 0.871"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = y_train[:2500]\n",
    "y_test = y_test[:500]\n",
    "# Преобразование данных в нужный формат\n",
    "X_train = X_train.reshape(60_000, 784)[:2500]\n",
    "X_test = X_test.reshape(10_000, 784)[:500]\n",
    "k = 550  # Количество компонент для сжатия\n",
    "\n",
    "# Epoch 0, Train Loss: 0.9871, Test Loss: 1.2458\n",
    "# Epoch 50, Train Loss: 0.0352, Test Loss: 0.3426\n",
    "# Epoch 100, Train Loss: 0.0124, Test Loss: 0.3376\n",
    "# Epoch 150, Train Loss: 0.0078, Test Loss: 0.3440\n",
    "# Epoch 200, Train Loss: 0.0056, Test Loss: 0.3446\n",
    "# Epoch 250, Train Loss: 0.0043, Test Loss: 0.3512\n",
    "# Epoch 300, Train Loss: 0.0035, Test Loss: 0.3525\n",
    "# Epoch 350, Train Loss: 0.0029, Test Loss: 0.3538\n",
    "# Epoch 400, Train Loss: 0.0025, Test Loss: 0.3561\n",
    "# Epoch 450, Train Loss: 0.0022, Test Loss: 0.3572\n",
    "# Epoch 500, Train Loss: 0.0019, Test Loss: 0.3612\n",
    "# CPU times: user 7h 26min 40s, sys: 2h 38min 41s, total: 10h 5min 21s\n",
    "Wall time: 2h 35min 33s\n",
    "\n",
    "\n",
    "Accuracy: 0.896"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}